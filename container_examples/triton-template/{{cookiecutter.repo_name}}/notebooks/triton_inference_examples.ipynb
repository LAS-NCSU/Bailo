{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import requests\n",
    "from mlserver.types import InferenceRequest, InferenceErrorResponse, InferenceResponse, RequestInput\n",
    "from mlserver.codecs import StringCodec, NumpyCodec, StringRequestCodec\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're deploying several versions of spacy we're going to create a folder for each with the relevant artifacts\n",
    "# Build will copy the folders out of the src directory and create the conda tar file to run our models on Triton\n",
    "!python utils build\n",
    "\n",
    "# Sklearn download example\n",
    "# !python utils.py download -spcy en_core_web_sm -o ../dist/model/spacy_ner_sm/1/artifacts --model-version 3.7.0\n",
    "\n",
    "# Download HuggingFace repo example\n",
    "# !python utils.py download -hf openai/clip-vit-large-patch14 -o ../dist/models/clip/1/artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Triton and loading models\n",
    "\n",
    "Here we'll start up a triton server with docker compose up, since we use the `--model-control-mode=explicit` option in the server startup Triton will not try to automatically load all models in it's model repo.  \n",
    "\n",
    "This is useful as it will let us choose when to load and unload models for testing, dramatically speeding up development and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts CPU version\n",
    "!docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List / Load / Unload a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available():\n",
    "    endpoint = f\"http://localhost:8080/v2/repository/models/index\"\n",
    "    return requests.post(endpoint)\n",
    "\n",
    "def get_server_health():\n",
    "    endpoints = {\"ready\": \"http://localhost:8080/v2/health/ready\", \"live\": \"http://localhost:8080/v2/health/live\"}\n",
    "    status = {\n",
    "        \"live\": False,\n",
    "        \"ready\" : False\n",
    "    }\n",
    "    for endpoint, url in endpoints.items():\n",
    "        if requests.get(url).status_code == 200:\n",
    "            status[endpoint] = True\n",
    "            continue\n",
    "\n",
    "    return status\n",
    "\n",
    "def unload_model(model_name):\n",
    "    endpoint = f\"http://localhost:8080/v2/repository/models/{model_name}/unload\"\n",
    "    return requests.post(endpoint)\n",
    "\n",
    "def load_model(model_name):\n",
    "    endpoint = f\"http://localhost:8080/v2/repository/models/{model_name}/load\"\n",
    "    return requests.post(endpoint)\n",
    "\n",
    "def get_model_stats(model_name):\n",
    "    endpoint = f\"http://localhost:8080/v2/models/{model_name}/stats\"\n",
    "    return requests.get(endpoint)\n",
    "\n",
    "def run_inference(model_name, _payload):\n",
    "    encoded_input = RequestInput(\n",
    "        name=\"INPUT__0\", \n",
    "        shape=[len(_payload)], \n",
    "        datatype=\"BYTES\", \n",
    "        parameters={\"content_type\": \"str\"}, \n",
    "        data=_payload\n",
    "    )\n",
    "\n",
    "    inference_request = InferenceRequest(\n",
    "                            id=1,\n",
    "                            inputs=[encoded_input], \n",
    "                            parameters={\"payload_type\": \"text\"}\n",
    "                        )\n",
    "\n",
    "    print('----- JSON dump of the V2 request -----')\n",
    "    print(json.dumps(inference_request.dict()))\n",
    "    \n",
    "    endpoint = f\"http://localhost:8080/v2/models/{model_name}/infer\"\n",
    "    return requests.post(endpoint, json=inference_request.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_server_health()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_available().text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model(\"{{cookiecutter.model_name}}\")\n",
    "print(get_available().text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference request for text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "payload = [\"Bill ate Robert and had stuff to do in Washington\"]\n",
    "\n",
    "response = run_inference(\"{{cookiecutter.model_name}}\", payload)\n",
    "\n",
    "print('\\n----- Server Unparsed Response -----')\n",
    "print(response.text)\n",
    "inf_response = InferenceResponse.parse_raw(response.text)\n",
    "\n",
    "print('\\n----- Server Parsed Response -----')\n",
    "print(f'Model Name: {inf_response.model_name}')\n",
    "print(f'Model Version: {inf_response.model_version}')\n",
    "parsed_resp = StringCodec().decode_output(inf_response.outputs[0])\n",
    "print(f'Response: {parsed_resp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_model_stats(\"{{cookiecutter.model_name}}\").json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_model(\"{{cookiecutter.model_name}}\")\n",
    "print(get_available().text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
